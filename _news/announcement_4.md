---
layout: post
date: 2025-2-27
inline: true
related_posts: false
---

[Pursuits and Challenges Towards Simulation-free Training of Neural Samplers](https://jiajunhe98.github.io/assets/pdf/Pursuits and Challenges Towards Simulation-free Training of Neural Samplers.pdf)\
*Sampling Reading Group at Mila*\
<sub>**Abstract:** We consider the sampling problem, where the aim is to draw samples from a distribution whose density is known only up to a normalization constant. Recent breakthroughs in generative modeling to approximate a high-dimensional data distribution have sparked significant interest in developing neural network-based methods for this challenging problem. However, neural samplers typically incur heavy computational overhead due to simulating trajectories during training. This motivates the pursuit of simulation-free training procedures of neural samplers. In this work, we propose an elegant modification to previous methods, which allows simulation-free training with the help of a time-dependent normalizing flow. However, it ultimately suffers from severe mode collapse. On closer inspection, we find that nearly all successful neural samplers rely on Langevin preconditioning to avoid mode collapsing. We systematically analyze several popular methods with various objective functions and demonstrate that, in the absence of Langevin preconditioning, most of them fail to adequately cover even a simple target. Finally, we draw attention to a strong baseline by combining the state-of-the-art MCMC method, Parallel Tempering (PT), with an additional generative model to shed light on future explorations of neural samplers. This talk is based on the paper: [No Trick, No Treat: Pursuits and Challenges Towards Simulation-free Training of Neural Samplers](https://arxiv.org/abs/2502.06685), and delivered jointly with [Yuanqi Du](https://yuanqidu.github.io/).</sub>